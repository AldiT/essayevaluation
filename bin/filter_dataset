#!/usr/bin/env python3
import argparse
"""
v1:
Input: * EFCAMDAT files
Output: one .xml containing n essays closest to the average token number
"""
try:
    from bs4 import BeautifulSoup as BS
    import spacy
    import numpy as np
except ImportError as err:
    print('Dependencies misssing. Please install the ' + str(err.name) + ' module')
    exit()


# spaCy pipeline components
class TokenCountPipelineComponent(object):
    name = "token_counter"

    def __init__(self):
        self.token_count = []

    def __call__(self, doc):
        self.token_count.append(len(doc))# __len__ get the number of tokens in the document
        return doc


def read_essays(input_files):
    """
    reads a list of xml files of the EFCAMDAT corpus
    :param input_files:
    :return: texts, grades, xml
    """
    essays = []
    grades = []
    xml = []
    for file in input_files:
        with open(file) as fh:
            soup = BS(fh, "lxml")
            writings = soup.find_all("writing")
            for writing in writings:
                textxml = writing.find("text")
                if textxml is not None:
                    essays.append(textxml.text)
                    grades.append(int(writing.find('grade').text))
                    xml.append(writing)
                else:
                    print("error while reading xml")
    return essays, grades, xml


def get_token_counts(essays):
    nlp = spacy.load('en_core_web_sm')

    # we only need the tokenizer from spaCy
    nlp.remove_pipe('tagger')
    nlp.remove_pipe('parser')
    nlp.remove_pipe('ner')

    token_counter = TokenCountPipelineComponent()
    nlp.add_pipe(token_counter, name=token_counter.name, last=True)

    docs = list(nlp.pipe(essays))

    return token_counter.token_count



def get_args():
    parser = argparse.ArgumentParser(description='This tool offers to filter the EFCAMDAT dataset based on specified '
                                                 'requirements. (e.g. same essay length, equal distributed scores, ...')
    parser.add_argument('input_files', nargs='+', help='an integer for the accumulator')
    parser.add_argument('output_file', help='an integer for the accumulator')
    parser.add_argument('--length', action='store_true', help='')
    parser.add_argument('-n', '--number', type=int, help='number of essays which shall be output', default=10000)

    return parser.parse_args()


def main():
    args = get_args()
    print("read essays")
    essays, grades, xml = read_essays(args.input_files)

    if len(essays) < args.number:
        print('input data is smaller or equal than requested number of essays (' + str(len(essays)) + ' <= ' +
              str(args.number) + ')')
        exit()

    print("count tokens")
    tk = get_token_counts(essays)
    token_counts = np.array(tk)
    mean_token_count = token_counts.mean()
    token_counts_diff = np.abs(token_counts - mean_token_count)

    index_count = np.stack((np.arange(len(essays)), token_counts_diff), axis=1)
    index_count_sorted = index_count[index_count[:, 1].argsort()]

    print("write result")
    with open(args.output_file, 'w') as fh:
        fh.write('<?xml version="1.0" encoding="UTF-8"?>\n')
        fh.write('<selection id="filtered_selection">\n')
        fh.write('<meta name="inputfiles">' + ','.join(args.input_files) + '</meta>\n')
        fh.write('<meta name="avg_token_count">' + str(mean_token_count) + '</meta>\n')
        fh.write('<writings>\n')

        for index, (essay_index, token_diff) in enumerate(index_count_sorted):
            if index >= args.number:
                break
            essay_xml = xml[int(essay_index)]

            fh.write(str(essay_xml) + '\n')

        fh.write('</writings>\n')
        fh.write('</selection>')


if __name__ == "__main__":
    main()
